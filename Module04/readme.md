Breakdown of Module 4: Model Evaluation from the Machine Learning Zoomcamp notebook:

Train/Test Split – Split your dataset into training and test sets to evaluate how your model performs on unseen data.

Model Training – Train a logistic regression model using the training data.

Predict Probabilities – Use .predict_proba() to get churn probabilities (e.g., how likely a customer is to churn).

Evaluation Metrics – Use metrics like accuracy, precision, recall, F1, and ROC AUC.

ROC Curve – Plot True Positive Rate vs False Positive Rate to visualize performance.

Choosing Thresholds – Learn how adjusting probability cutoffs changes accuracy and recall.

Cross-Validation – Use multiple folds to get stable, averaged performance scores.

Final Evaluation – Select the model and threshold that balance performance metrics best.
